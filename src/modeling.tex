Data-driven modeling of a complex physical system such as a building generally consists of: (1) selecting the model type, (2) constructing the model structure (based on certain insights of the system), (3) optimally designing and running experiments to obtain data, and (4) training and validating the model from data.
In general, the more data we have, the better we can learn a model using machine learning algorithms.
However, as discussed in Section~\ref{sec:framework}, for many physical systems, the amount of training data we can practically obtain is usually limited due to many factors.
In addition, operation and safety constraints in such a physical system typically require deterministic or probabilistic guarantees on the performance of the model and the closed-loop control system.
Consequently, it is desirable to select a machine learning model type that is highly flexible, that works well with limited data sets, and that can provide estimates of model uncertainty.
In this work, Gaussian Processes (GPs) are chosen, as they possess all these properties.
Section~\ref{sec:modeling:gp} gives a brief introduction to GPs, followed by a detailed description of how building systems are modeled by GPs in Section~\ref{sec:modeling:building}.
In Section~\ref{sec:modeling:oed}, we present our method for optimal experiment design with GPs.

\subsection{Brief Tutorial of Gaussian Processes}
\label{sec:modeling:gp}

This section, adapted from \cite{JainICCPS2018}, briefly introduces Gaussian Processes (GP).
For an in-depth treatment of GP, interested readers are referred to \cite{Rasmussen2006}.

Consider noisy observations \(y\) of an underlying function \(f: \RR^n \mapsto \RR\) through a Gaussian noise model: \(y = f(x) + \epsilon\), where \(\epsilon \sim \GaussianDist{0}{\sigma_n^2}\) and  \(x \in \RR^n\).
A GP of $y$ is essentially a probability distribution on the observations of all possible realizations of $f$.
By conditioning the GP on a finite set of observation data of $f$, one can derive the posterior distribution, which allows probabilistic inference at a new input.
% The formal definition of a GP can be found in \cite{Rasmussen2006} and is reproduced below.
% \begin{definition}
% A Gaussian Process is a collection of random variables, any finite number of which have a joint Gaussian distribution.
% \end{definition}
%
A GP of \(y\) is fully specified by its mean function \(\mu(x)\) and covariance function \(k(x,x')\), parameterized by the \emph{hyperparameters} \(\theta\),
\begin{align*}
% \label{eq:gp-prior}
\mu(x; \theta) &= \EE [f(x)] \\
k(x,x'; \theta) &= \EE [(f(x)\!-\!\mu(x)) (f(x') \!-\! \mu(x'))] + \sigma_n^2 \delta(x,x') \nonumber
\end{align*}
where \(\delta(x,x')\) is the Kronecker delta function.
There exists a wide range of covariance functions and combinations to choose from \cite{Rasmussen2006}.

Given the regression vectors \(X = [x_1, \dots, x_N]^T\) and the corresponding observed outputs \(Y = [y_1, \dots, y_N]^T\).
Unlike many other machine learning methods, with a GP, the output \(y_\star\) corresponding to an input \(x_\star\) is a random variable with a Gaussian distribution \(y_{\star} \sim \GaussianDist{\bar{y}_\star}{\sigma_\star^2}\), where
\begin{subequations}
\label{eq:gp-regression}
\begin{align}
\bar{y}_\star &= \mu(x_\star) + K_\star K^{-1} (Y - \mu(X))\\
\sigma_\star^2 &= K_{\star \star} - K_\star K^{-1} K_\star^T \text,
\end{align}
\end{subequations}
in which \(K_\star = [k(x_\star, x_1), \dots, k(x_\star, x_N)]\), \(K_{\star \star} = k(x_\star, x_\star)\), and $K$ is the covariance matrix with elements \(K_{ij} = k(x_i, x_j)\).
Note that the mean and variance are nonlinear in $x_{\star}$ and their computations scale cubically with the size of $X$ and $Y$.

In theory, the hyperparameters $\theta$ are also random variables, which define the GP distribution on the function and whose posterior distributions are obtained by conditioning them on $(X,Y)$ using the Bayes' theorem.
In practice, $\theta$ are often obtained by maximizing the likelihood: \(\argmax_\theta \Pr(Y \vert X, \theta)\).

The covariance function \(k(x,x')\) indicates how correlated the outputs are at \(x\) and \(x'\), with the intuition that the output at an input is influenced more by the outputs of nearby inputs in the training data. 
In other words, a GP model specifies the structure of the covariance matrix of, or the relationship between, the input variables rather than a fixed structural input--output relationship.
A GP is therefore \emph{highly flexible} and \emph{can capture complex behavior with fewer parameters}.
Consequently, GPs generally \emph{work well with small data sets}, which is useful for any learning application where data are not easily obtained.
A GP also provides an \emph{estimate of uncertainty or doubt} in the predictions through the predictive variance, which can be used to assess or guarantee the performance of a learning-based system.
For these advantages, GPs are selected for our framework.

\subsection{Learning Models of Building Energy Systems} % with Gaussian Processes}
\label{sec:modeling:building}

A buiding is a complex and large-scale physical system.
The energy system of a medium-sized building may have thousands of variables related to temperature, pressure, flow, power, and other physical quantities.
In a smart building, these variables are usually managed by the building's SCADA system, which contains a hierarchy of control loops that regulate these variables in all components of the energy system.
This work focuses on high-level supervisory control of smart buildings in two important aspects: electrical energy consumption (specifically total power demand) and occupant comfort (specifically temperature in building zones).

Given a building system, we aim to model its power demand and zone temperature behavior from data that can be measured directly from installed sensors such as thermostats, multimeters and weather forecast.
For this purpose, two types of models are learned: \(\mathcal{M}_{\mathrm{P}}\) for predicting the power demand and \(\mathcal{M}_{\mathrm{T}}\) for predicting the temperature of a particular zone in the building.
The following variables are used for learning these models:
\begin{itemize}
\item \textit{Control variables \(u\):} These variables influence how key control loops in a building energy system, such as Air-Handling Unitd (AHU) and chillers, operate.  They are selected based on which inputs we can and want to manipulate in order to achieve the desired behavior of the building, such as cooling, supply air temperature and chilled water temperature setpoints.  They will be optimized in our control algorithms.
\item \textit{Output variable \(y\):} total power consumption for \(\mathcal{M}_{\mathrm{P}}\) or zone temperature for \(\mathcal{M}_{\mathrm{T}}\) -- this is the output of interest, to be predicted using all the above variables.
\item \textit{Disturbance variables \(d\):} These are variables which we do not have control over, or do not want to control, but definitely have influence on the interested aspect of the building (power demand in the case of $\mathcal{M}_{\mathrm{P}}$ and zone temperature in the case of $\mathcal{M}_{\mathrm{T}}$).  They typically include:
  \begin{itemize}
  \item \emph{Weather variables} such as outside air temperature and humidity, obtained from historical weather data.
  \item \emph{Temporal variables} such as time of day and day of week, which indicate occupancy, system operation schedules, and periodic trends.
  \item \emph{Fixed schedules} such as preset setpoints that we do not control and therefore are not included in the control variables $u$.
  \end{itemize}
\end{itemize}
These variables, called features for the machine learning models, are the most influential to the power usage and thermal comfort in a building, among all building system variables.
They constitute a set of variables that achieve a good balance between modeling accuracy on one hand and modeling simplicity and cost on the other hand.


As can be seen from the above notations, we classify these variables into control inputs \(u\), exogenous disturbance inputs \(d\), and output \(y\).
Because building systems are dynamic, we feed time-delayed, also known as \emph{autoregressive}, input and output signals to the models as features to account for the memory effect of the dynamic systems.
In particular, at time step $t$, the regressor vector $x_{t}$, \ie the complete input vector of the model, would be
\begin{equation}
  \label{E:GP:features}
  x_{t}\!=\![y_{t-l}, \dots, y_{t-1}, u_{t-m}, \dots, u_t, d_{t-p}, \dots, d_{t-1}, d_t] \text,
\end{equation}
where \(l\), \(m\), and \(p\) are respectively the lags for autoregressive outputs, control inputs, and disturbances.

Given training data of input and output pairs $\{x_{t}, y_{t}\}$, a model can be learned.
%
In this work, we model the power dynamics ($\mathcal{M}_{\mathrm{P}}$) and temperature dynamics ($\mathcal{M}_{\mathrm{T}}$) with GPs for their ability to work well with small datasets and to provide estimates of predictive uncertainty.
Specifically, $\mathcal{M}_{\mathrm{P}}$ or $\mathcal{M}_{\mathrm{T}}$ is a GP of the respective output
\begin{math}
  y_{t} \sim \GaussianDist{\bar{y}\left(x_{t}\right)}{\sigma^{2}\left(x_{t}\right)} \text.
\end{math}

For $\mathcal{M}_{\mathrm{P}}$, a constant mean function and the covariance function proposed in our previous work \cite{nghiemetal16gp} are used.
The covariance function is specially designed to capture both the temporal pattern of the energy usage and the effect of non-temporal features on the power demand.
It comprises a constant \(k_{1}\), a squared exponential kernel \(k_2(x,x')\), and a rational quadratic kernel \(k_{3}(x,x')\) as
%
\begin{gather}
  k(x,x') = \left(k_1 + k_2(x,x')\right)*k_3(x,x')\text,\nonumber\\
  k_2(x,x') = \sigma_{f_2}^2 \exp \left( -\textstyle\frac{1}{2} \textstyle\sum_{d=1}^D \frac{(x_d-x_d')}{{\lambda_d^2}}^2 \right)\text,
 \nonumber\\
 k_3(x,x') = \sigma_{f_3}^2  \left( 1+ \textstyle\frac{1}{2\alpha} \textstyle\sum_{d=1}^D \frac{(x_d-x_d')}{{\lambda^2}}^2 \right)^{-\alpha}\text,
\end{gather}
%
where \(D\) is the dimension of \(x\).
The rational of this covariance function design, as discussed in \cite{nghiemetal16gp}, is that:
(a) $k_{1}$ represents the base power demand;
(b) $k_{3}$ captures the temporal pattern of the energy usage and therefore only applies to the proxy variables $d^{p}$; and
(c) $k_{2}$ captures the influence of non-temporal features on the power demand.

A GP model $\mathcal{M}_{\mathrm{T}}$ for the temperature of a particular zone can be obtained in a similar manner.


% \begin{figure}[t]
% 	\centering
% 	\setlength\fwidth{0.43\textwidth} 
% 	\setlength\hwidth{0.22\textwidth}
% 	\input{figures/prediction_TotalLoad.tex}
% 	\caption{Rolling forecast of the power demand for one particular day using GP model \(\mathcal{M}_1\). The actual demand is almost always within the 95\% confidence interval.}
% 	\label{F:prediction}
% \end{figure}

\subsection{Optimal Experiment Design} % with Gaussian Processes}
\label{sec:modeling:oed}

As discussed above, in many practical applications, it is crucial to maximize the quality of the data used for learning a system's model with a specific technique, meaning that the achievable model quality is optimal given all the constraints imposed on the functional testing process, including but not limited to time constraints and actuation constraints.
This practice is often known as \emph{optimal experiment design} (OED).
We present in this section an \emph{information-theoretic} approach to OED based on GPs.

An important feature of GPs, used in our work to model physical systems, is the availability of predictive variances, which provide quantitative measures of the uncertainty or doubt in their predictions.
By exploiting these predictive variances, one can estimate the Information Gain (IG) induced by adding a training data point to an existing GP model \cite{Krause2008} \note{May not need this ref}.
Maximizing the IG allows us to incrementally design a functional test, namely excitation signals, to obtain training data that best explain the behavior of the underlying physical system with GP.

Let us denote the existing training data as \(\D\).
Let the posterior over the hyperparameters be \(\theta|\D \sim \GaussianDist{\bar{\theta}}{\Sigma}\) and, at a new input $x$, let \(y|x \sim \GaussianDist{\bar{y}(x)}{\sigma^2(x);\theta}\).
The IG approach selects the next sample $x$ to add the maximum information to the model, that is to optimally reduce the uncertainty in $\theta$ or equivalently to maximize the IG defined as
\begin{align}
\textstyle\argmax_x H(\theta|\D) - \EE_{y \sim \GaussianDist{\bar{y}(x)}{\sigma^2(x)}}H(\theta|\D,x,y)\text,
\label{E:ig:theta}
\end{align}
where \(H\) is the Shannon's Entropy given by
\begin{math}
H(\theta|\D) = -\int p(\theta|\D) \log (p(\theta|\D))d\theta \text.
\end{math}
We can find a linear approximation to \(\bar{y}(x) = a^T(x)\theta+b(x)\) such that
\begin{equation*}
p(y|x,\D) \sim \GaussianDist{a^T\bar{\theta}+b}{\sigma^2+a^T\Sigma a}
\end{equation*}
in the neighborhood of \(\bar{\theta}\).
It can be shown that, under this approximation, maximizing the IG in \eqref{E:ig:theta} is equivalent to maximizing \(\tilde{\sigma}^2(x)/{\sigma}^2(x)\), where
\begin{align*}
\tilde{\sigma}^2(x) = &\frac{4}{3}\sigma^2(x) + \frac{\partial \bar{y}(x)}{\partial \theta}^T \Sigma \frac{\partial \bar{y}(x)}{\partial \theta} + \nonumber \\
& \frac{1}{3\sigma^2(x)}\frac{\partial \sigma^2(x)}{\partial \theta}^T \Sigma \frac{\partial \sigma^2(x)}{\partial \theta}
\end{align*}
evaluated at \(\bar{\theta}\).
More details of the mathematical derivation can be found in our previous work~\cite{JainICCPS2018}.

Back to the dynamical GP model in~\ref{sec:modeling:building}, at time $t$, our goal is to find the unknown control input $u_{t} \in \RR^{n_{u}}$ in the regressor vector $x_{t}$ in \eqref{E:GP:features} to maximize \(\tilde{\sigma}^2(x_{t})/{\sigma}^2(x_{t})\).
Note that all the features in $x_{t}$ except for $u_{t}$ are known at time $t$.
For physical systems, very often, we must operate under actuation or operation constraints.
Let $\mathcal{U}_{t}$ denote the constraints on $u_{t}$.
We seek to compute an optimal control input \(u^*_t = \argmin_{u_{t} \in \mathcal{U}_{t}} \tilde{\sigma}^2(x_t)/{\sigma}^2(x_t) \), which is applied to the physical system, resulting in the output $y_{t}$.
The pair $(x_{t}, y_{t})$ is added to $\D$ and the hyperparameters \(\theta\) are updated with maximum a posteriori (MAP) estimate, % \cite{Garnett2013},
then we proceed to time \(t+1\). 
The proposed algorithm for OED is summarized in Algorithm \ref{A:oed:sequential}.
\begin{algorithm}[!tb]
	\caption{Sequential sampling for OED based on IG}
	\label{A:oed:sequential}
	\begin{algorithmic}[1]
		% \Procedure{Initialization}{}
		% \If{initial \(\D := (X,Y)\)}
		% \State Compute \( \theta_{\mathrm{MLE}} = \argmax_{\theta^{\mathrm{MLE}}} \Pr(Y \vert X, \theta)\)
		% \State Assign priors \(\theta_{\mathrm{0}} \sim \GaussianDist{\theta_{\mathrm{MLE}}}{\sigma^2_{\mathrm{init}}}\)
		% \Else 
		% \State Assign priors \(\theta_{\mathrm{0}} \sim \GaussianDist{\mu_{\mathrm{{init}}}}{\sigma^2_\mathrm{init}}\)
		% \EndIf
		% \EndProcedure
		\Procedure{OED-Sampling}{}
		\While{\(t<t_{\mathrm{max}}\)}
		\State Construct features \(x_t\) in \eqref{E:GP:features} as a function of \(u_t\)
		\State \(u^*_t \leftarrow \argmin_{u_{t} \in \mathcal{U}_{t}} \tilde{\sigma}^2(x_t)/{\sigma}^2(x_t) \)
		\State Apply \(u^*_t\) to the system and obtainj \(y_t\)
		\State \(\D \leftarrow \D \cup (x_t,y_t) \)
		\State Update \( \theta_{t} \leftarrow \argmax_{\theta^{\mathrm{MAP}}} \Pr(Y \vert X, \theta_{\mathrm{t-1}})\)
		\EndWhile
		\EndProcedure
      \end{algorithmic}
\end{algorithm}


% As an example, in Sec.~\ref{S:casestudy} where we learn a dynamical model of a building, the proposed OED method is used to optimally sample the chilled water temperature, supply air temperature and zone-level cooling set-points, subject to operation constraints on the chiller system.
% The result is illustrated in Fig.~\ref{F:oed:example}, which shows the changes in model accuracy between several experiment design methods, for various durations of functional tests.
% For short functional test durations, our OED methods achieve much more accurate models compared to random sampling methods. The random sampling requires \(2 \times\) time to reach the same accuracy as OED. 
% Since the historical data is unperturbed, it would require even longer to provide similar accuracy.

% \begin{figure}[!tb]
%   \centering
%   \setlength\fwidth{0.4\textwidth}
%   \setlength\hwidth{0.18\textwidth}	
%   \input{figures/oed-acc.tex}
%   \caption{Error in prediction of power consumption on test dataset. RMSE denotes root mean square error and AE denotes absolute error. The errors are much lower with optimal experiment design based on information gain. The random sampling requires \(2 \times\) time to reach the same accuracy as OED.}
%   % \captionsetup{justification=centering}
%   \vspace{-12pt}
%   \label{F:oed:example}
% \end{figure}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
